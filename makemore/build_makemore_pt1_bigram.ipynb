{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61171c66",
   "metadata": {},
   "source": [
    "# Goal explanation (character-level Bigram language model)\n",
    "\n",
    "In makemore part 1, we are creating a character-level language model using names.txt as input data.\n",
    "\n",
    "The language model simply takes the input data, trains based on it, and makes likely predictions based on the training. Since the language model is a \"character-level\" model, we predict 1 character based on input. For this session, we are training a \"bi-gram\" model, which means we train sets of 2-characters. 1 earlier char is the \"before\" part and the 2nd char (last char) is the \"output\" char. If we have N-gram instead, we would be using (N-1) as the \"before\" sequence of characters and the N^th (last) char as the \"output\".\n",
    "\n",
    "What `character-level` means is that for each input data, the training treats each character as unit of training. For example, even if 1 line of input sample is \"hello\", the training occurs for each 2 characters (bi-gram) pairs. In this case it would be (h,e), (e,l), (l,l), (l,o) for 4 sets of training input from this 1 line.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17675549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c82e2345",
   "metadata": {},
   "source": [
    "# TODO: input data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c594a8f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f9bc4e3",
   "metadata": {},
   "source": [
    "# TODO: input data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f22cd55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ed8a88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c80040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df197e5d",
   "metadata": {},
   "source": [
    "# approach 1 : counting bigrams approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23766df2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ddc38d5",
   "metadata": {},
   "source": [
    "# approach 2 : neural net approach to char-level bigram language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8ff267",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
